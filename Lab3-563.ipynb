{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhikr487/Rproject/blob/main/Lab3-563.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKwgFHVvRMql"
      },
      "source": [
        "# COLX 563 Lab Assignment 3: Question-Answering with BERT\n",
        "## Assignment Objectives\n",
        "\n",
        "In this lab, you will implement and train a (distil)BERT model for Question and Answering on a subset of the [SQuAD v2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset. Lab objectives include:\n",
        "\n",
        "1. Convert the data to tensors using the BERT tokenizer\n",
        "2. Train a model for Question-Answering by tuning on top of a pre-trained BERT model\n",
        "3. Optimize the choice of start and end indices\n",
        "\n",
        "We use distBERT in this lab because it is significantly smaller and faster than BERT, but with very similar performance. Even though we are using distBERT, we will call it BERT throughout this lab\n",
        "\n",
        "You will likely need a GPU for this lab, but the free version of Colab should be sufficient.  The code to mount your drive on Colab is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fg3d9lgd-NT",
        "outputId": "fc897c2c-0277-4040-e293-1fc4cda46803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.8.0-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.8.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pulp\n",
        "!{sys.executable} -m pip install transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVe9XpXPRMqm"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "Run the code below to access relevant modules (you can add to this as needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aXd_5YurRMqm"
      },
      "outputs": [],
      "source": [
        "#provided code\n",
        "import numpy as np\n",
        "import torch\n",
        "import pulp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmzOERS4RMqq"
      },
      "source": [
        "For this lab, you'll be working with the SQuAD database. Download the SQuAD data from [the repo](https://github.ubc.ca/MDS-CL-2023-24/COLX_563_adv-semantics_students/tree/master/data/lab3), unzip it into a directory outside of your lab repo and change the path below. Later you will probably want to put the data on Google drive and change this path so it points to your mounted data.\n",
        "\n",
        "The question, context (also called the passage), and answer for a given set of QA training data are stored in separate files with corresponding line numbers. You should open up the data files to make sure you understand what they each represent."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Lab3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B71haO3r5dEO",
        "outputId": "a8bb0bd5-a691-492c-80b3-311199d9fed1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/gdrive/MyDrive/'Colab Notebooks'/Lab3.ipynb /content/gdrive/MyDrive/Lab3"
      ],
      "metadata": {
        "id": "TXzXH3Zv48xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RZ9_ABDURMqq"
      },
      "outputs": [],
      "source": [
        "#provided code\n",
        "squad_path = '/content/gdrive/MyDrive/Lab3/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "3ALw_1avzzqI",
        "outputId": "89bdfa3a-938c-40fe-f224-d6293f46ae8b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e79c22b5-f1b3-4e39-8a50-e8245f42045f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e79c22b5-f1b3-4e39-8a50-e8245f42045f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.question to test.question\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "torch.backends.cudnn.deterministic=True\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7pV08CT5wI9",
        "outputId": "50e0ddc8-58bd-4862-bd1a-e1b1c34364f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO_gf_eSdkRx",
        "outputId": "2a4a1512-bd7a-449c-a991-5edfe93a5ae1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EhCTfduRMqt"
      },
      "source": [
        "## Tidy Submission\n",
        "rubric={mechanics:1}\n",
        "\n",
        "To get the marks for tidy submission:\n",
        "- Submit the assignment by filling in this Jupyter notebook with your answers embedded\n",
        "- Be sure to follow the instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Ei4yBlRMqt"
      },
      "source": [
        "## Exercise 1: Initial Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ob53ujRMqu"
      },
      "source": [
        "### Exercise 1.1\n",
        "rubric={accuracy:2}\n",
        "\n",
        "Your first task is to write a function, `convert_to_BERT_tensors`, which uses the build-in BERT tokenizer to create tensors for input to the BERT model. You should call the tokenizer directly, look at the tokenizer [docs](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__). This function should involve only two lines of code, but you need to get the arguments right. Your tokenization process must\n",
        "* return pytorch tensors corresponding to the input_ids and attention masks (which prevent BERT from attending to padding)\n",
        "* combine questions and contexts into a single input with a separator character\n",
        "* truncate when the question and context is too long to work with BERT (longer than 512)\n",
        "* add padding when the question and context is too short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SP00GzbiRMqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "db0efb44c88d4b2ca847dc57ee9a7616",
            "350b6b4cb9ea43dc8cf84b46410affe4",
            "a9f8b6a986954f388580aa27d38f4541",
            "6ab7b9d218764991914c8accb2446045",
            "10cf0efbd8d24c7aad7be5f1a16b5b2a",
            "4e4b49d5ea6f4c74a318eb96fd705a0d",
            "0d0fa97de72d4f51a2081b3921aab4c0",
            "cdb6a3c5564a4df1a4b7b465870a87e2",
            "5a7f4611b54b4973ba313e7dfe380ab9",
            "a429155131e34595939507f1122e6d8f",
            "9034be4769804ec5b220bd4617f35a9b",
            "42ffc530955a4cfaa2bd32f4bed08b42",
            "acaaa5d0625c43d998e975f9c9b6b551",
            "87f468138ffc4884acb4fcf4ea0b0409",
            "c26226e928234d1f9f700db4e3285f69",
            "e6d3613560d34b7e871b60cf2af9fe5a",
            "89e61dd4d25b42c58a3e622ae8e615b2",
            "50d5b5790a9d44ecba5a587de4b898d8",
            "9d7c60a4943f4297807b3d099a5d84f3",
            "ad32882649d344899fe5866a7d96a037",
            "64e61084154c4f78b0b1b8ea1a30df41",
            "1edf6d730130464c91e79fd902d4b535",
            "92dfe776feda48aea3d95b7d9280810b",
            "ddc2b7705ad54d7b99a5d98094610628",
            "02393b909e854145b8069fae46b9888c",
            "b45bc83c30004b878de748bf63575733",
            "8f9c725caa7843ed8583145eb2fa1fa5",
            "a7e599bb428549a5bc25595099317be3",
            "15754d92befa49a0801e69b7b1dd631c",
            "02dd6ae7c74f4a8c850423b57036b1be",
            "5a4c49e57d29429391b36954fa09ef01",
            "9164932a6f384bc797714068b09fef58",
            "72fec13a789149e4bb355e263d71b982",
            "45b12882f5b1485b8a21c854c390d6ae",
            "72543cf3a9064c19bce891399ad5ce84",
            "0ccf35c1a1ee410fa45d453615c471f2",
            "5d376bf0dc1e409e8f147461c4ba841d",
            "842044bb6708415cbfb2e72142692e07",
            "1e81f5c53ec747aa8a3f899bb7dc3915",
            "a2b5a5a34781460290f0e93804264fa8",
            "fb67124fc3824246813a53f446d2d23b",
            "e0e664dc338a4cca89a037d4a4cdc278",
            "47dc0209a5c2437fa25caf1208fa5c0c",
            "51d945bcf5a14f92a2b130ef03e2000b"
          ]
        },
        "outputId": "06737d09-ca37-4893-c0e0-d3f303a3a8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db0efb44c88d4b2ca847dc57ee9a7616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42ffc530955a4cfaa2bd32f4bed08b42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92dfe776feda48aea3d95b7d9280810b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45b12882f5b1485b8a21c854c390d6ae"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "\n",
        "def convert_to_BERT_tensors(questions, contexts):\n",
        "    '''takes a parallel list of question strings and answer strings'''\n",
        "\n",
        "    paell_lst = tokenizer(questions, contexts, return_tensors='pt', truncation=True, padding=True)\n",
        "    return paell_lst['input_ids'], paell_lst['attention_mask']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_fq2PNTRMqx",
        "outputId": "cf6ae865-272c-4e4e-d1f4-9f8d58c2646a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "test_questions = [\"Why?\", \"How?\"]\n",
        "test_contexts = [\"I think it is because we can bluminate\", \"It was done\"\" \".join([\"very\"]*1000) + \" well\"]\n",
        "\n",
        "ids, mask = convert_to_BERT_tensors(test_questions,test_contexts)\n",
        "assert ids.shape == (2,512) # 512 because that's the max allowed\n",
        "assert ids[0][3] == 102 # fourth token is separator\n",
        "assert list(ids[0][-100:]) == [0]*100 # first row is mostly padding\n",
        "assert list(ids[1][-100:]) != [0]*100 # second row is not\n",
        "assert list(mask[0][-100:]) == [0]*100 # first row padding is masked\n",
        "assert list(mask[1][-100:]) != [0]*100 # second row is not padding, no mask\n",
        "print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYW5MOfqRMq2"
      },
      "source": [
        "### Exercise 1.2\n",
        "rubric={accuracy:3, efficiency:1}\n",
        "\n",
        "Creating tensors for the answers is a bit trickier. As our target for training, we want tensors of indices which correspond to the beginning and end of the answer span. Since BERT uses a special tokenizer and puts the question and the context together into a single input sequence, the original answer spans provided by SQuAD (which correspond to regular tokens in the context) aren't useful.\n",
        "\n",
        "Instead you will write a function called `get_answer_span_tensor` which takes strings corresponding to a question, context, and answer and identifies the location of the answer in the vector corresponding to the input (question + context, including the required special BERT tokens \\[CLS\\] and \\[SEP\\]). To accomplish this, you will want to use the [tokenize](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.tokenize) method of the tokenizer, which just does the tokenization, not the vectorization (i.e. creation of tensor representation, already accomplished in exercise 1). Apply `tokenize` to both answer string and the input string, and then match the answer within the input string to get the indicies. Note that the end index should be inclusive, i.e if your indices were 34 and 36, that would correspond to a 3 word answer, not a 2 word answer.\n",
        "\n",
        "Note, if the answer does not appear in the input, you should set start and end indices both to zero. Remember that in 1.1. you are truncating the input to length 512 so if one of the indices ends outside of 512, you should treat it as a failed match (this is important, if you don't do this, your code will crash in Exercise 2!)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1GwfsTGKRMq3"
      },
      "outputs": [],
      "source": [
        "def get_answer_span_tensor(question_text, context_text, answer_text):\n",
        "    # Tokenize the question and context texts together with special tokens to separate them\n",
        "    input_tokens = tokenizer.tokenize('[CLS] ' + question_text + ' [SEP] ' + context_text)\n",
        "\n",
        "    # Tokenize the answer text separately\n",
        "    answer_tokens = tokenizer.tokenize(answer_text)\n",
        "\n",
        "    # Calculate the length of the answer tokens list\n",
        "    span_length = len(answer_tokens)\n",
        "\n",
        "    # Loop through the input tokens to find the matching sequence for the answer tokens\n",
        "    for i in range(min(len(input_tokens) - span_length + 1, 512 - span_length - 1)):\n",
        "        # Check if the current slice of input tokens matches the answer tokens\n",
        "        if input_tokens[i:i + span_length] == answer_tokens:\n",
        "            # If a match is found, create a tensor representing the start and end indices of the span\n",
        "            span = torch.tensor([i, i + span_length - 1])\n",
        "            break\n",
        "    else:\n",
        "        # If no match is found, return a tensor representing an invalid span\n",
        "        span = torch.tensor([0, 0])\n",
        "\n",
        "    # Return the span tensor\n",
        "    return span\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgFinF9_RMq5",
        "outputId": "491480b7-7224-4e18-fb18-a86a31cea7aa",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "test_question = \"Why?\"\n",
        "test_context = \"I think it is because we can bluminate\"\n",
        "test_answer = \"because we can bluminate\"\n",
        "bad_answer  = \"because we can fumiage\"\n",
        "span = get_answer_span_tensor(test_question,test_context,test_answer)\n",
        "assert span.shape == (2,)\n",
        "assert list(span) == [8,12]\n",
        "span = get_answer_span_tensor(test_question,test_context,bad_answer)\n",
        "assert list(span) == [0,0]\n",
        "print('Success!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUoukTskd-NW"
      },
      "source": [
        "### Exercise 1.3\n",
        "rubric={accuracy:2, quality:1}\n",
        "\n",
        "Now write code that builds a `QAdataset` (defined below) and a corresponding dataloader for each of the train, dev, and test splits with the provided `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n_ZMN2__d-NW"
      },
      "outputs": [],
      "source": [
        "#provided code\n",
        "batch_size = 16\n",
        "\n",
        "class QAdataset(Dataset):\n",
        "    '''A dataset for housing QA data, including input_data, output_data, and padding mask'''\n",
        "    def __init__(self, input_data, output_data,mask):\n",
        "        self.input_data = input_data\n",
        "        self.output_data = output_data\n",
        "        self.mask = mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.output_data[index]\n",
        "        data_val = self.input_data[index]\n",
        "        mask = self.mask[index]\n",
        "        return data_val,target,mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDy5H7p6d-NX"
      },
      "source": [
        "You will want to open the corresponding question, context, and answers files for each split and use the functions in 1.1 and 1.2 to create tensors to be passed to the QAdataset constructor. Note that you don't have answers for the test split but you can just default to a (0,0) target in that case. Don't be surprised if your dataloaders take in the order of 10 minutes to build (it is a large dataset!)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_QA_dataset(split):\n",
        "    with open(squad_path + split + \".question\", encoding=\"utf-8\") as file_q:\n",
        "        questions = file_q.readlines()\n",
        "    with open(squad_path + split + \".context\", encoding=\"utf-8\") as file_c:\n",
        "        contexts = file_c.readlines()\n",
        "    input_tensors, masks = convert_to_BERT_tensors(questions, contexts)\n",
        "\n",
        "    if \"train\" == split or \"dev\" == split:\n",
        "        with open(squad_path + split + \".answer\", encoding=\"utf-8\") as file_a:\n",
        "            answers = file_a.readlines()\n",
        "            spans = []\n",
        "            for i in range(len(questions)):\n",
        "                spans.append(get_answer_span_tensor(questions[i], contexts[i], answers[i]))\n",
        "    else:\n",
        "        spans = [torch.tensor([0,0])]*len(questions)\n",
        "    return QAdataset(input_tensors, spans, masks)"
      ],
      "metadata": {
        "id": "S9nKERriIYWJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvs-WFMC-Bdm",
        "outputId": "ee932044-cc05-49a4-f442-c0fb16e71136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mIee06gG-JjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = prepare_QA_dataset(\"train\")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "dev_dataset = prepare_QA_dataset('dev')\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataset = prepare_QA_dataset('test')\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ywjqQObIf9o",
        "outputId": "7a989de7-10b2-4bc6-f7ed-efd9e98767ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2MmQ7bCD-enp",
        "outputId": "573c559f-1014-4337-a461-5f10bc0469bf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Lab3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDrQP9NyRMrS"
      },
      "source": [
        "## Exercise 2: BERT Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPaXZFwfRMrS"
      },
      "source": [
        "### Exercise 2.1\n",
        "rubric={accuracy:2}\n",
        "\n",
        "The Huggingface library has a [BERT QA model](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforquestionanswering) that includes the main pre-trained BERT model as well as the QA heads, i.e. start and end embeddings, which are dot-producted with the embedding output of BERT for each token to get unnormalized probabilities indicating which token is most likely to be the starting and ending token. Note there is actually one available that has been pre-tuned on SQuaD, but we won't be using that here. Instead, we load the `DistilBertForQuestionAnswering` module with a regular DistilBert pre-trained model `distilbert-base-uncased` (which could be used for any task), and teach it to do QA (it will give you a warning about that this model of BERT has not been trained for QA, don't worry about this!). The initialization is provided for you below. You need to set up an appropriate loss function and optimizer; for the latter, use Adam with a learning rate of 0.00003. Then, iterate over the data using your dataloader, passing the inputs and masks to the forward function of the model in the usual way, and then calculate the loss for both start and end \"logits\" (the unnormalized probability that each token is the start/end of the answer) which form part of the output of the model. Print out the loss regularly, if everything is correct you should see it drop *very* fast. You only need to train your model for a single epoch here (you may want to increase this number later for the Kaggle competition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "74a909e8259649acb4460e841c4b3a44",
            "80c9238632a54c07aa6971d6a239d6f8",
            "1d695de50a0742929f794b6b4ad480b2",
            "9973d359bd53494e952e394e71330342",
            "2ee2cbcb5ed9421ebaf82e69675da116",
            "a813d46881fc4fd0a2cb9558553caed9",
            "c14df920692a4cf1bf3af48e16b33dd5",
            "469e3e991bb64a6f99e1084f0303daf3",
            "8817c0928ba548aa8c0fcb48cb985a27",
            "5f218da0c32b460483642caeab395fac",
            "eace7a097f3b47eb9a2b71b52f3a4472"
          ]
        },
        "id": "jDzJrSVwRMrT",
        "outputId": "a5255ace-72da-4253-8fa4-fbe9cd7558e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74a909e8259649acb4460e841c4b3a44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed  160 QA pairs of  77558\n",
            "Last loss: 11.438800811767578\n",
            "Processed  320 QA pairs of  77558\n",
            "Last loss: 9.11563491821289\n",
            "Processed  480 QA pairs of  77558\n",
            "Last loss: 9.385068893432617\n",
            "Processed  640 QA pairs of  77558\n",
            "Last loss: 7.9191107749938965\n",
            "Processed  800 QA pairs of  77558\n",
            "Last loss: 8.244309425354004\n",
            "Processed  960 QA pairs of  77558\n",
            "Last loss: 6.816885948181152\n",
            "Processed  1120 QA pairs of  77558\n",
            "Last loss: 6.925061225891113\n",
            "Processed  1280 QA pairs of  77558\n",
            "Last loss: 6.212676048278809\n",
            "Processed  1440 QA pairs of  77558\n",
            "Last loss: 6.955818176269531\n",
            "Processed  1600 QA pairs of  77558\n",
            "Last loss: 6.984383583068848\n",
            "Processed  1760 QA pairs of  77558\n",
            "Last loss: 6.755611419677734\n",
            "Processed  1920 QA pairs of  77558\n",
            "Last loss: 4.889864921569824\n",
            "Processed  2080 QA pairs of  77558\n",
            "Last loss: 5.028020858764648\n",
            "Processed  2240 QA pairs of  77558\n",
            "Last loss: 5.240192890167236\n",
            "Processed  2400 QA pairs of  77558\n",
            "Last loss: 5.477156639099121\n",
            "Processed  2560 QA pairs of  77558\n",
            "Last loss: 4.5737128257751465\n",
            "Processed  2720 QA pairs of  77558\n",
            "Last loss: 3.04516863822937\n",
            "Processed  2880 QA pairs of  77558\n",
            "Last loss: 4.368963718414307\n",
            "Processed  3040 QA pairs of  77558\n",
            "Last loss: 3.568228006362915\n",
            "Processed  3200 QA pairs of  77558\n",
            "Last loss: 3.9603681564331055\n",
            "Processed  3360 QA pairs of  77558\n",
            "Last loss: 2.854336738586426\n",
            "Processed  3520 QA pairs of  77558\n",
            "Last loss: 3.547475814819336\n",
            "Processed  3680 QA pairs of  77558\n",
            "Last loss: 3.2107348442077637\n",
            "Processed  3840 QA pairs of  77558\n",
            "Last loss: 2.2467329502105713\n",
            "Processed  4000 QA pairs of  77558\n",
            "Last loss: 3.4654881954193115\n",
            "Processed  4160 QA pairs of  77558\n",
            "Last loss: 4.283470153808594\n",
            "Processed  4320 QA pairs of  77558\n",
            "Last loss: 3.5547361373901367\n",
            "Processed  4480 QA pairs of  77558\n",
            "Last loss: 2.9138171672821045\n",
            "Processed  4640 QA pairs of  77558\n",
            "Last loss: 3.3435218334198\n",
            "Processed  4800 QA pairs of  77558\n",
            "Last loss: 4.031347274780273\n",
            "Processed  4960 QA pairs of  77558\n",
            "Last loss: 4.166428565979004\n",
            "Processed  5120 QA pairs of  77558\n",
            "Last loss: 3.605034112930298\n",
            "Processed  5280 QA pairs of  77558\n",
            "Last loss: 4.8614044189453125\n",
            "Processed  5440 QA pairs of  77558\n",
            "Last loss: 3.721348524093628\n",
            "Processed  5600 QA pairs of  77558\n",
            "Last loss: 2.354948043823242\n",
            "Processed  5760 QA pairs of  77558\n",
            "Last loss: 3.0186877250671387\n",
            "Processed  5920 QA pairs of  77558\n",
            "Last loss: 3.5124096870422363\n",
            "Processed  6080 QA pairs of  77558\n",
            "Last loss: 3.041017532348633\n",
            "Processed  6240 QA pairs of  77558\n",
            "Last loss: 3.337778091430664\n",
            "Processed  6400 QA pairs of  77558\n",
            "Last loss: 3.4598422050476074\n",
            "Processed  6560 QA pairs of  77558\n",
            "Last loss: 4.142162322998047\n",
            "Processed  6720 QA pairs of  77558\n",
            "Last loss: 3.820352554321289\n",
            "Processed  6880 QA pairs of  77558\n",
            "Last loss: 3.9074573516845703\n",
            "Processed  7040 QA pairs of  77558\n",
            "Last loss: 1.1576018333435059\n",
            "Processed  7200 QA pairs of  77558\n",
            "Last loss: 3.7581570148468018\n",
            "Processed  7360 QA pairs of  77558\n",
            "Last loss: 2.4612441062927246\n",
            "Processed  7520 QA pairs of  77558\n",
            "Last loss: 4.953817844390869\n",
            "Processed  7680 QA pairs of  77558\n",
            "Last loss: 3.292581558227539\n",
            "Processed  7840 QA pairs of  77558\n",
            "Last loss: 2.943427562713623\n",
            "Processed  8000 QA pairs of  77558\n",
            "Last loss: 2.9907712936401367\n",
            "Processed  8160 QA pairs of  77558\n",
            "Last loss: 2.1579132080078125\n",
            "Processed  8320 QA pairs of  77558\n",
            "Last loss: 3.793148994445801\n",
            "Processed  8480 QA pairs of  77558\n",
            "Last loss: 3.2505364418029785\n",
            "Processed  8640 QA pairs of  77558\n",
            "Last loss: 2.334071159362793\n",
            "Processed  8800 QA pairs of  77558\n",
            "Last loss: 2.4515466690063477\n",
            "Processed  8960 QA pairs of  77558\n",
            "Last loss: 3.062734365463257\n",
            "Processed  9120 QA pairs of  77558\n",
            "Last loss: 3.4071617126464844\n",
            "Processed  9280 QA pairs of  77558\n",
            "Last loss: 2.672995090484619\n",
            "Processed  9440 QA pairs of  77558\n",
            "Last loss: 3.2613375186920166\n",
            "Processed  9600 QA pairs of  77558\n",
            "Last loss: 3.1134724617004395\n",
            "Processed  9760 QA pairs of  77558\n",
            "Last loss: 4.6434454917907715\n",
            "Processed  9920 QA pairs of  77558\n",
            "Last loss: 2.5708532333374023\n",
            "Processed  10080 QA pairs of  77558\n",
            "Last loss: 3.658572196960449\n",
            "Processed  10240 QA pairs of  77558\n",
            "Last loss: 1.964223861694336\n",
            "Processed  10400 QA pairs of  77558\n",
            "Last loss: 3.1209254264831543\n",
            "Processed  10560 QA pairs of  77558\n",
            "Last loss: 3.1419501304626465\n",
            "Processed  10720 QA pairs of  77558\n",
            "Last loss: 3.68312406539917\n",
            "Processed  10880 QA pairs of  77558\n",
            "Last loss: 3.4159979820251465\n",
            "Processed  11040 QA pairs of  77558\n",
            "Last loss: 4.017328262329102\n",
            "Processed  11200 QA pairs of  77558\n",
            "Last loss: 2.8967089653015137\n",
            "Processed  11360 QA pairs of  77558\n",
            "Last loss: 3.3753416538238525\n",
            "Processed  11520 QA pairs of  77558\n",
            "Last loss: 3.2332119941711426\n",
            "Processed  11680 QA pairs of  77558\n",
            "Last loss: 3.7103726863861084\n",
            "Processed  11840 QA pairs of  77558\n",
            "Last loss: 4.03740119934082\n",
            "Processed  12000 QA pairs of  77558\n",
            "Last loss: 4.825530052185059\n",
            "Processed  12160 QA pairs of  77558\n",
            "Last loss: 4.498870849609375\n",
            "Processed  12320 QA pairs of  77558\n",
            "Last loss: 2.829085350036621\n",
            "Processed  12480 QA pairs of  77558\n",
            "Last loss: 1.938202977180481\n",
            "Processed  12640 QA pairs of  77558\n",
            "Last loss: 2.522007942199707\n",
            "Processed  12800 QA pairs of  77558\n",
            "Last loss: 3.1230435371398926\n",
            "Processed  12960 QA pairs of  77558\n",
            "Last loss: 2.7329912185668945\n",
            "Processed  13120 QA pairs of  77558\n",
            "Last loss: 1.5493812561035156\n",
            "Processed  13280 QA pairs of  77558\n",
            "Last loss: 4.29728364944458\n",
            "Processed  13440 QA pairs of  77558\n",
            "Last loss: 3.2323131561279297\n",
            "Processed  13600 QA pairs of  77558\n",
            "Last loss: 2.65087890625\n",
            "Processed  13760 QA pairs of  77558\n",
            "Last loss: 3.0226922035217285\n",
            "Processed  13920 QA pairs of  77558\n",
            "Last loss: 1.3022551536560059\n",
            "Processed  14080 QA pairs of  77558\n",
            "Last loss: 3.5923104286193848\n",
            "Processed  14240 QA pairs of  77558\n",
            "Last loss: 3.0099711418151855\n",
            "Processed  14400 QA pairs of  77558\n",
            "Last loss: 2.9454479217529297\n",
            "Processed  14560 QA pairs of  77558\n",
            "Last loss: 3.9077975749969482\n",
            "Processed  14720 QA pairs of  77558\n",
            "Last loss: 2.9308042526245117\n",
            "Processed  14880 QA pairs of  77558\n",
            "Last loss: 2.786803722381592\n",
            "Processed  15040 QA pairs of  77558\n",
            "Last loss: 2.834378957748413\n",
            "Processed  15200 QA pairs of  77558\n",
            "Last loss: 3.0000133514404297\n",
            "Processed  15360 QA pairs of  77558\n",
            "Last loss: 2.7081689834594727\n",
            "Processed  15520 QA pairs of  77558\n",
            "Last loss: 3.268618106842041\n",
            "Processed  15680 QA pairs of  77558\n",
            "Last loss: 3.062687397003174\n",
            "Processed  15840 QA pairs of  77558\n",
            "Last loss: 2.232959270477295\n",
            "Processed  16000 QA pairs of  77558\n",
            "Last loss: 2.2616782188415527\n",
            "Processed  16160 QA pairs of  77558\n",
            "Last loss: 4.4412522315979\n",
            "Processed  16320 QA pairs of  77558\n",
            "Last loss: 3.4900827407836914\n",
            "Processed  16480 QA pairs of  77558\n",
            "Last loss: 3.055091381072998\n",
            "Processed  16640 QA pairs of  77558\n",
            "Last loss: 2.9471421241760254\n",
            "Processed  16800 QA pairs of  77558\n",
            "Last loss: 2.626605749130249\n",
            "Processed  16960 QA pairs of  77558\n",
            "Last loss: 4.795928955078125\n",
            "Processed  17120 QA pairs of  77558\n",
            "Last loss: 3.587839126586914\n",
            "Processed  17280 QA pairs of  77558\n",
            "Last loss: 2.040174961090088\n",
            "Processed  17440 QA pairs of  77558\n",
            "Last loss: 3.661154270172119\n",
            "Processed  17600 QA pairs of  77558\n",
            "Last loss: 3.3326826095581055\n",
            "Processed  17760 QA pairs of  77558\n",
            "Last loss: 2.7171592712402344\n",
            "Processed  17920 QA pairs of  77558\n",
            "Last loss: 5.155817031860352\n",
            "Processed  18080 QA pairs of  77558\n",
            "Last loss: 2.2828574180603027\n",
            "Processed  18240 QA pairs of  77558\n",
            "Last loss: 3.8942606449127197\n",
            "Processed  18400 QA pairs of  77558\n",
            "Last loss: 3.3857736587524414\n",
            "Processed  18560 QA pairs of  77558\n",
            "Last loss: 2.4380245208740234\n",
            "Processed  18720 QA pairs of  77558\n",
            "Last loss: 3.1422884464263916\n",
            "Processed  18880 QA pairs of  77558\n",
            "Last loss: 2.125209331512451\n",
            "Processed  19040 QA pairs of  77558\n",
            "Last loss: 3.2280445098876953\n",
            "Processed  19200 QA pairs of  77558\n",
            "Last loss: 3.5034027099609375\n",
            "Processed  19360 QA pairs of  77558\n",
            "Last loss: 2.6616411209106445\n",
            "Processed  19520 QA pairs of  77558\n",
            "Last loss: 2.8025221824645996\n",
            "Processed  19680 QA pairs of  77558\n",
            "Last loss: 3.0804033279418945\n",
            "Processed  19840 QA pairs of  77558\n",
            "Last loss: 4.204227447509766\n",
            "Processed  20000 QA pairs of  77558\n",
            "Last loss: 3.1335744857788086\n",
            "Processed  20160 QA pairs of  77558\n",
            "Last loss: 4.099205017089844\n",
            "Processed  20320 QA pairs of  77558\n",
            "Last loss: 4.147566318511963\n",
            "Processed  20480 QA pairs of  77558\n",
            "Last loss: 2.978226661682129\n",
            "Processed  20640 QA pairs of  77558\n",
            "Last loss: 2.0240211486816406\n",
            "Processed  20800 QA pairs of  77558\n",
            "Last loss: 1.9937052726745605\n",
            "Processed  20960 QA pairs of  77558\n",
            "Last loss: 3.968615770339966\n",
            "Processed  21120 QA pairs of  77558\n",
            "Last loss: 2.7757105827331543\n",
            "Processed  21280 QA pairs of  77558\n",
            "Last loss: 2.484647750854492\n",
            "Processed  21440 QA pairs of  77558\n",
            "Last loss: 2.9305620193481445\n",
            "Processed  21600 QA pairs of  77558\n",
            "Last loss: 3.3915212154388428\n",
            "Processed  21760 QA pairs of  77558\n",
            "Last loss: 4.623032569885254\n",
            "Processed  21920 QA pairs of  77558\n",
            "Last loss: 2.398080348968506\n",
            "Processed  22080 QA pairs of  77558\n",
            "Last loss: 3.508627414703369\n",
            "Processed  22240 QA pairs of  77558\n",
            "Last loss: 2.675844430923462\n",
            "Processed  22400 QA pairs of  77558\n",
            "Last loss: 2.838845729827881\n",
            "Processed  22560 QA pairs of  77558\n",
            "Last loss: 2.9838106632232666\n",
            "Processed  22720 QA pairs of  77558\n",
            "Last loss: 3.483463764190674\n",
            "Processed  22880 QA pairs of  77558\n",
            "Last loss: 2.6607067584991455\n",
            "Processed  23040 QA pairs of  77558\n",
            "Last loss: 2.6549057960510254\n",
            "Processed  23200 QA pairs of  77558\n",
            "Last loss: 2.445188522338867\n",
            "Processed  23360 QA pairs of  77558\n",
            "Last loss: 3.3727781772613525\n",
            "Processed  23520 QA pairs of  77558\n",
            "Last loss: 2.829289197921753\n",
            "Processed  23680 QA pairs of  77558\n",
            "Last loss: 2.9428482055664062\n",
            "Processed  23840 QA pairs of  77558\n",
            "Last loss: 2.214996337890625\n",
            "Processed  24000 QA pairs of  77558\n",
            "Last loss: 1.9689111709594727\n",
            "Processed  24160 QA pairs of  77558\n",
            "Last loss: 2.567500114440918\n",
            "Processed  24320 QA pairs of  77558\n",
            "Last loss: 3.2035605907440186\n",
            "Processed  24480 QA pairs of  77558\n",
            "Last loss: 3.0151093006134033\n",
            "Processed  24640 QA pairs of  77558\n",
            "Last loss: 4.189363479614258\n",
            "Processed  24800 QA pairs of  77558\n",
            "Last loss: 4.017878532409668\n",
            "Processed  24960 QA pairs of  77558\n",
            "Last loss: 3.5375313758850098\n",
            "Processed  25120 QA pairs of  77558\n",
            "Last loss: 2.9295215606689453\n",
            "Processed  25280 QA pairs of  77558\n",
            "Last loss: 2.9282608032226562\n",
            "Processed  25440 QA pairs of  77558\n",
            "Last loss: 2.952638626098633\n",
            "Processed  25600 QA pairs of  77558\n",
            "Last loss: 1.4802337884902954\n",
            "Processed  25760 QA pairs of  77558\n",
            "Last loss: 1.33588445186615\n",
            "Processed  25920 QA pairs of  77558\n",
            "Last loss: 2.7338151931762695\n",
            "Processed  26080 QA pairs of  77558\n",
            "Last loss: 2.6845455169677734\n",
            "Processed  26240 QA pairs of  77558\n",
            "Last loss: 3.368795394897461\n",
            "Processed  26400 QA pairs of  77558\n",
            "Last loss: 2.552438735961914\n",
            "Processed  26560 QA pairs of  77558\n",
            "Last loss: 3.2746033668518066\n",
            "Processed  26720 QA pairs of  77558\n",
            "Last loss: 2.903846025466919\n",
            "Processed  26880 QA pairs of  77558\n",
            "Last loss: 1.744682788848877\n",
            "Processed  27040 QA pairs of  77558\n",
            "Last loss: 3.052253246307373\n",
            "Processed  27200 QA pairs of  77558\n",
            "Last loss: 2.484027147293091\n",
            "Processed  27360 QA pairs of  77558\n",
            "Last loss: 3.4055628776550293\n",
            "Processed  27520 QA pairs of  77558\n",
            "Last loss: 2.6894168853759766\n",
            "Processed  27680 QA pairs of  77558\n",
            "Last loss: 2.1294779777526855\n",
            "Processed  27840 QA pairs of  77558\n",
            "Last loss: 2.2530698776245117\n",
            "Processed  28000 QA pairs of  77558\n",
            "Last loss: 4.404531955718994\n",
            "Processed  28160 QA pairs of  77558\n",
            "Last loss: 2.808043956756592\n",
            "Processed  28320 QA pairs of  77558\n",
            "Last loss: 2.2241904735565186\n",
            "Processed  28480 QA pairs of  77558\n",
            "Last loss: 2.672258138656616\n",
            "Processed  28640 QA pairs of  77558\n",
            "Last loss: 2.507082939147949\n",
            "Processed  28800 QA pairs of  77558\n",
            "Last loss: 3.0907578468322754\n",
            "Processed  28960 QA pairs of  77558\n",
            "Last loss: 1.801543951034546\n",
            "Processed  29120 QA pairs of  77558\n",
            "Last loss: 2.7446231842041016\n",
            "Processed  29280 QA pairs of  77558\n",
            "Last loss: 4.734589576721191\n",
            "Processed  29440 QA pairs of  77558\n",
            "Last loss: 2.4695565700531006\n",
            "Processed  29600 QA pairs of  77558\n",
            "Last loss: 2.057908773422241\n",
            "Processed  29760 QA pairs of  77558\n",
            "Last loss: 2.939866542816162\n",
            "Processed  29920 QA pairs of  77558\n",
            "Last loss: 2.106114387512207\n",
            "Processed  30080 QA pairs of  77558\n",
            "Last loss: 2.067671775817871\n",
            "Processed  30240 QA pairs of  77558\n",
            "Last loss: 3.2786824703216553\n",
            "Processed  30400 QA pairs of  77558\n",
            "Last loss: 2.7748732566833496\n",
            "Processed  30560 QA pairs of  77558\n",
            "Last loss: 3.401714324951172\n",
            "Processed  30720 QA pairs of  77558\n",
            "Last loss: 3.972471237182617\n",
            "Processed  30880 QA pairs of  77558\n",
            "Last loss: 4.510455131530762\n",
            "Processed  31040 QA pairs of  77558\n",
            "Last loss: 3.3646647930145264\n",
            "Processed  31200 QA pairs of  77558\n",
            "Last loss: 3.2608232498168945\n",
            "Processed  31360 QA pairs of  77558\n",
            "Last loss: 3.4765186309814453\n",
            "Processed  31520 QA pairs of  77558\n",
            "Last loss: 1.7791094779968262\n",
            "Processed  31680 QA pairs of  77558\n",
            "Last loss: 3.565483808517456\n",
            "Processed  31840 QA pairs of  77558\n",
            "Last loss: 1.8731659650802612\n",
            "Processed  32000 QA pairs of  77558\n",
            "Last loss: 2.562177896499634\n",
            "Processed  32160 QA pairs of  77558\n",
            "Last loss: 3.323225259780884\n",
            "Processed  32320 QA pairs of  77558\n",
            "Last loss: 3.485316753387451\n",
            "Processed  32480 QA pairs of  77558\n",
            "Last loss: 4.576935768127441\n",
            "Processed  32640 QA pairs of  77558\n",
            "Last loss: 1.3337408304214478\n",
            "Processed  32800 QA pairs of  77558\n",
            "Last loss: 3.0960681438446045\n",
            "Processed  32960 QA pairs of  77558\n",
            "Last loss: 1.8508726358413696\n",
            "Processed  33120 QA pairs of  77558\n",
            "Last loss: 1.8587095737457275\n",
            "Processed  33280 QA pairs of  77558\n",
            "Last loss: 3.0137624740600586\n",
            "Processed  33440 QA pairs of  77558\n",
            "Last loss: 2.1877524852752686\n",
            "Processed  33600 QA pairs of  77558\n",
            "Last loss: 2.851114273071289\n",
            "Processed  33760 QA pairs of  77558\n",
            "Last loss: 2.293104410171509\n",
            "Processed  33920 QA pairs of  77558\n",
            "Last loss: 1.5039687156677246\n",
            "Processed  34080 QA pairs of  77558\n",
            "Last loss: 1.5228910446166992\n",
            "Processed  34240 QA pairs of  77558\n",
            "Last loss: 2.0596837997436523\n",
            "Processed  34400 QA pairs of  77558\n",
            "Last loss: 1.8655856847763062\n",
            "Processed  34560 QA pairs of  77558\n",
            "Last loss: 3.3852977752685547\n",
            "Processed  34720 QA pairs of  77558\n",
            "Last loss: 2.414334774017334\n",
            "Processed  34880 QA pairs of  77558\n",
            "Last loss: 3.2251858711242676\n",
            "Processed  35040 QA pairs of  77558\n",
            "Last loss: 1.771078109741211\n",
            "Processed  35200 QA pairs of  77558\n",
            "Last loss: 1.9393795728683472\n",
            "Processed  35360 QA pairs of  77558\n",
            "Last loss: 2.714057445526123\n",
            "Processed  35520 QA pairs of  77558\n",
            "Last loss: 2.4894747734069824\n",
            "Processed  35680 QA pairs of  77558\n",
            "Last loss: 2.4085116386413574\n",
            "Processed  35840 QA pairs of  77558\n",
            "Last loss: 1.3838605880737305\n",
            "Processed  36000 QA pairs of  77558\n",
            "Last loss: 1.7098033428192139\n",
            "Processed  36160 QA pairs of  77558\n",
            "Last loss: 2.6644511222839355\n",
            "Processed  36320 QA pairs of  77558\n",
            "Last loss: 2.575437545776367\n",
            "Processed  36480 QA pairs of  77558\n",
            "Last loss: 1.8600871562957764\n",
            "Processed  36640 QA pairs of  77558\n",
            "Last loss: 2.5289320945739746\n",
            "Processed  36800 QA pairs of  77558\n",
            "Last loss: 2.108314037322998\n",
            "Processed  36960 QA pairs of  77558\n",
            "Last loss: 1.3670990467071533\n",
            "Processed  37120 QA pairs of  77558\n",
            "Last loss: 2.9259262084960938\n",
            "Processed  37280 QA pairs of  77558\n",
            "Last loss: 1.8234729766845703\n",
            "Processed  37440 QA pairs of  77558\n",
            "Last loss: 2.11559796333313\n",
            "Processed  37600 QA pairs of  77558\n",
            "Last loss: 3.6270523071289062\n",
            "Processed  37760 QA pairs of  77558\n",
            "Last loss: 1.5768167972564697\n",
            "Processed  37920 QA pairs of  77558\n",
            "Last loss: 4.98541259765625\n",
            "Processed  38080 QA pairs of  77558\n",
            "Last loss: 3.003023147583008\n",
            "Processed  38240 QA pairs of  77558\n",
            "Last loss: 3.0620975494384766\n",
            "Processed  38400 QA pairs of  77558\n",
            "Last loss: 1.2944879531860352\n",
            "Processed  38560 QA pairs of  77558\n",
            "Last loss: 3.831007242202759\n",
            "Processed  38720 QA pairs of  77558\n",
            "Last loss: 2.6974921226501465\n",
            "Processed  38880 QA pairs of  77558\n",
            "Last loss: 2.905731678009033\n",
            "Processed  39040 QA pairs of  77558\n",
            "Last loss: 2.317521810531616\n",
            "Processed  39200 QA pairs of  77558\n",
            "Last loss: 2.3742353916168213\n",
            "Processed  39360 QA pairs of  77558\n",
            "Last loss: 3.7852556705474854\n",
            "Processed  39520 QA pairs of  77558\n",
            "Last loss: 3.0886902809143066\n",
            "Processed  39680 QA pairs of  77558\n",
            "Last loss: 2.6173741817474365\n",
            "Processed  39840 QA pairs of  77558\n",
            "Last loss: 1.8350520133972168\n",
            "Processed  40000 QA pairs of  77558\n",
            "Last loss: 2.84671688079834\n",
            "Processed  40160 QA pairs of  77558\n",
            "Last loss: 3.061227798461914\n",
            "Processed  40320 QA pairs of  77558\n",
            "Last loss: 2.8159074783325195\n",
            "Processed  40480 QA pairs of  77558\n",
            "Last loss: 2.8511815071105957\n",
            "Processed  40640 QA pairs of  77558\n",
            "Last loss: 2.138829231262207\n",
            "Processed  40800 QA pairs of  77558\n",
            "Last loss: 3.0417957305908203\n",
            "Processed  40960 QA pairs of  77558\n",
            "Last loss: 1.8389198780059814\n",
            "Processed  41120 QA pairs of  77558\n",
            "Last loss: 3.747344732284546\n",
            "Processed  41280 QA pairs of  77558\n",
            "Last loss: 1.5975310802459717\n",
            "Processed  41440 QA pairs of  77558\n",
            "Last loss: 3.3631954193115234\n",
            "Processed  41600 QA pairs of  77558\n",
            "Last loss: 2.7879271507263184\n",
            "Processed  41760 QA pairs of  77558\n",
            "Last loss: 3.4827933311462402\n",
            "Processed  41920 QA pairs of  77558\n",
            "Last loss: 2.278062105178833\n",
            "Processed  42080 QA pairs of  77558\n",
            "Last loss: 4.266469955444336\n",
            "Processed  42240 QA pairs of  77558\n",
            "Last loss: 1.6489958763122559\n",
            "Processed  42400 QA pairs of  77558\n",
            "Last loss: 2.830108642578125\n",
            "Processed  42560 QA pairs of  77558\n",
            "Last loss: 2.9794211387634277\n",
            "Processed  42720 QA pairs of  77558\n",
            "Last loss: 2.749833583831787\n",
            "Processed  42880 QA pairs of  77558\n",
            "Last loss: 1.6459745168685913\n",
            "Processed  43040 QA pairs of  77558\n",
            "Last loss: 2.1970057487487793\n",
            "Processed  43200 QA pairs of  77558\n",
            "Last loss: 1.2456722259521484\n",
            "Processed  43360 QA pairs of  77558\n",
            "Last loss: 4.618979454040527\n",
            "Processed  43520 QA pairs of  77558\n",
            "Last loss: 1.9860308170318604\n",
            "Processed  43680 QA pairs of  77558\n",
            "Last loss: 3.352966785430908\n",
            "Processed  43840 QA pairs of  77558\n",
            "Last loss: 2.9821958541870117\n",
            "Processed  44000 QA pairs of  77558\n",
            "Last loss: 2.285313129425049\n",
            "Processed  44160 QA pairs of  77558\n",
            "Last loss: 1.3126219511032104\n",
            "Processed  44320 QA pairs of  77558\n",
            "Last loss: 3.3279786109924316\n",
            "Processed  44480 QA pairs of  77558\n",
            "Last loss: 2.958631992340088\n",
            "Processed  44640 QA pairs of  77558\n",
            "Last loss: 1.5454045534133911\n",
            "Processed  44800 QA pairs of  77558\n",
            "Last loss: 3.2272074222564697\n",
            "Processed  44960 QA pairs of  77558\n",
            "Last loss: 1.153400182723999\n",
            "Processed  45120 QA pairs of  77558\n",
            "Last loss: 3.013345718383789\n",
            "Processed  45280 QA pairs of  77558\n",
            "Last loss: 3.0863699913024902\n",
            "Processed  45440 QA pairs of  77558\n",
            "Last loss: 1.8509279489517212\n",
            "Processed  45600 QA pairs of  77558\n",
            "Last loss: 2.718801259994507\n",
            "Processed  45760 QA pairs of  77558\n",
            "Last loss: 1.6756155490875244\n",
            "Processed  45920 QA pairs of  77558\n",
            "Last loss: 1.4414300918579102\n",
            "Processed  46080 QA pairs of  77558\n",
            "Last loss: 1.9627294540405273\n",
            "Processed  46240 QA pairs of  77558\n",
            "Last loss: 3.710188865661621\n",
            "Processed  46400 QA pairs of  77558\n",
            "Last loss: 2.2566606998443604\n",
            "Processed  46560 QA pairs of  77558\n",
            "Last loss: 3.393444776535034\n",
            "Processed  46720 QA pairs of  77558\n",
            "Last loss: 2.9500913619995117\n",
            "Processed  46880 QA pairs of  77558\n",
            "Last loss: 1.8847397565841675\n",
            "Processed  47040 QA pairs of  77558\n",
            "Last loss: 1.8890217542648315\n",
            "Processed  47200 QA pairs of  77558\n",
            "Last loss: 2.2317378520965576\n",
            "Processed  47360 QA pairs of  77558\n",
            "Last loss: 2.033677816390991\n",
            "Processed  47520 QA pairs of  77558\n",
            "Last loss: 2.163762092590332\n",
            "Processed  47680 QA pairs of  77558\n",
            "Last loss: 1.428071141242981\n",
            "Processed  47840 QA pairs of  77558\n",
            "Last loss: 3.39163875579834\n",
            "Processed  48000 QA pairs of  77558\n",
            "Last loss: 1.9907562732696533\n",
            "Processed  48160 QA pairs of  77558\n",
            "Last loss: 2.388885736465454\n",
            "Processed  48320 QA pairs of  77558\n",
            "Last loss: 1.9103960990905762\n",
            "Processed  48480 QA pairs of  77558\n",
            "Last loss: 3.183185577392578\n",
            "Processed  48640 QA pairs of  77558\n",
            "Last loss: 2.4848740100860596\n",
            "Processed  48800 QA pairs of  77558\n",
            "Last loss: 1.552871584892273\n",
            "Processed  48960 QA pairs of  77558\n",
            "Last loss: 1.6706314086914062\n",
            "Processed  49120 QA pairs of  77558\n",
            "Last loss: 2.6151084899902344\n",
            "Processed  49280 QA pairs of  77558\n",
            "Last loss: 2.4195380210876465\n",
            "Processed  49440 QA pairs of  77558\n",
            "Last loss: 2.5227885246276855\n",
            "Processed  49600 QA pairs of  77558\n",
            "Last loss: 2.9128456115722656\n",
            "Processed  49760 QA pairs of  77558\n",
            "Last loss: 1.1788675785064697\n",
            "Processed  49920 QA pairs of  77558\n",
            "Last loss: 2.899704933166504\n",
            "Processed  50080 QA pairs of  77558\n",
            "Last loss: 1.8017327785491943\n",
            "Processed  50240 QA pairs of  77558\n",
            "Last loss: 0.7578482031822205\n",
            "Processed  50400 QA pairs of  77558\n",
            "Last loss: 2.1243720054626465\n",
            "Processed  50560 QA pairs of  77558\n",
            "Last loss: 3.5134310722351074\n",
            "Processed  50720 QA pairs of  77558\n",
            "Last loss: 2.447051763534546\n",
            "Processed  50880 QA pairs of  77558\n",
            "Last loss: 1.689261794090271\n",
            "Processed  51040 QA pairs of  77558\n",
            "Last loss: 3.045334815979004\n",
            "Processed  51200 QA pairs of  77558\n",
            "Last loss: 1.8151822090148926\n",
            "Processed  51360 QA pairs of  77558\n",
            "Last loss: 4.131870269775391\n",
            "Processed  51520 QA pairs of  77558\n",
            "Last loss: 2.0471646785736084\n",
            "Processed  51680 QA pairs of  77558\n",
            "Last loss: 2.922243595123291\n",
            "Processed  51840 QA pairs of  77558\n",
            "Last loss: 3.2820968627929688\n",
            "Processed  52000 QA pairs of  77558\n",
            "Last loss: 2.366218090057373\n",
            "Processed  52160 QA pairs of  77558\n",
            "Last loss: 1.6321433782577515\n",
            "Processed  52320 QA pairs of  77558\n",
            "Last loss: 2.096223831176758\n",
            "Processed  52480 QA pairs of  77558\n",
            "Last loss: 2.175266742706299\n",
            "Processed  52640 QA pairs of  77558\n",
            "Last loss: 2.7300491333007812\n",
            "Processed  52800 QA pairs of  77558\n",
            "Last loss: 2.4429922103881836\n",
            "Processed  52960 QA pairs of  77558\n",
            "Last loss: 1.5512056350708008\n",
            "Processed  53120 QA pairs of  77558\n",
            "Last loss: 4.6327009201049805\n",
            "Processed  53280 QA pairs of  77558\n",
            "Last loss: 1.5543760061264038\n",
            "Processed  53440 QA pairs of  77558\n",
            "Last loss: 3.3230364322662354\n",
            "Processed  53600 QA pairs of  77558\n",
            "Last loss: 1.6707170009613037\n",
            "Processed  53760 QA pairs of  77558\n",
            "Last loss: 3.2448863983154297\n",
            "Processed  53920 QA pairs of  77558\n",
            "Last loss: 2.027844190597534\n",
            "Processed  54080 QA pairs of  77558\n",
            "Last loss: 2.632024049758911\n",
            "Processed  54240 QA pairs of  77558\n",
            "Last loss: 2.7228243350982666\n",
            "Processed  54400 QA pairs of  77558\n",
            "Last loss: 3.335005760192871\n",
            "Processed  54560 QA pairs of  77558\n",
            "Last loss: 2.014707565307617\n",
            "Processed  54720 QA pairs of  77558\n",
            "Last loss: 2.6524620056152344\n",
            "Processed  54880 QA pairs of  77558\n",
            "Last loss: 2.637235164642334\n",
            "Processed  55040 QA pairs of  77558\n",
            "Last loss: 2.0247836112976074\n",
            "Processed  55200 QA pairs of  77558\n",
            "Last loss: 2.026167631149292\n",
            "Processed  55360 QA pairs of  77558\n",
            "Last loss: 2.2900798320770264\n",
            "Processed  55520 QA pairs of  77558\n",
            "Last loss: 2.074321746826172\n",
            "Processed  55680 QA pairs of  77558\n",
            "Last loss: 2.001917839050293\n",
            "Processed  55840 QA pairs of  77558\n",
            "Last loss: 3.9936470985412598\n",
            "Processed  56000 QA pairs of  77558\n",
            "Last loss: 3.0802814960479736\n",
            "Processed  56160 QA pairs of  77558\n",
            "Last loss: 1.883009672164917\n",
            "Processed  56320 QA pairs of  77558\n",
            "Last loss: 2.763302803039551\n",
            "Processed  56480 QA pairs of  77558\n",
            "Last loss: 1.479679822921753\n",
            "Processed  56640 QA pairs of  77558\n",
            "Last loss: 1.9501550197601318\n",
            "Processed  56800 QA pairs of  77558\n",
            "Last loss: 1.5266516208648682\n",
            "Processed  56960 QA pairs of  77558\n",
            "Last loss: 2.4565842151641846\n",
            "Processed  57120 QA pairs of  77558\n",
            "Last loss: 2.0138604640960693\n",
            "Processed  57280 QA pairs of  77558\n",
            "Last loss: 1.9459176063537598\n",
            "Processed  57440 QA pairs of  77558\n",
            "Last loss: 3.650042772293091\n",
            "Processed  57600 QA pairs of  77558\n",
            "Last loss: 1.390961766242981\n",
            "Processed  57760 QA pairs of  77558\n",
            "Last loss: 1.8518686294555664\n",
            "Processed  57920 QA pairs of  77558\n",
            "Last loss: 2.6493520736694336\n",
            "Processed  58080 QA pairs of  77558\n",
            "Last loss: 1.4903509616851807\n",
            "Processed  58240 QA pairs of  77558\n",
            "Last loss: 1.7857296466827393\n",
            "Processed  58400 QA pairs of  77558\n",
            "Last loss: 2.9124317169189453\n",
            "Processed  58560 QA pairs of  77558\n",
            "Last loss: 1.8051021099090576\n",
            "Processed  58720 QA pairs of  77558\n",
            "Last loss: 1.1736233234405518\n",
            "Processed  58880 QA pairs of  77558\n",
            "Last loss: 1.1456247568130493\n",
            "Processed  59040 QA pairs of  77558\n",
            "Last loss: 1.5983209609985352\n",
            "Processed  59200 QA pairs of  77558\n",
            "Last loss: 3.261475086212158\n",
            "Processed  59360 QA pairs of  77558\n",
            "Last loss: 1.6684584617614746\n",
            "Processed  59520 QA pairs of  77558\n",
            "Last loss: 3.637754201889038\n",
            "Processed  59680 QA pairs of  77558\n",
            "Last loss: 2.111940622329712\n",
            "Processed  59840 QA pairs of  77558\n",
            "Last loss: 1.7546844482421875\n",
            "Processed  60000 QA pairs of  77558\n",
            "Last loss: 2.8272833824157715\n",
            "Processed  60160 QA pairs of  77558\n",
            "Last loss: 3.2717928886413574\n",
            "Processed  60320 QA pairs of  77558\n",
            "Last loss: 1.8888719081878662\n",
            "Processed  60480 QA pairs of  77558\n",
            "Last loss: 2.005499839782715\n",
            "Processed  60640 QA pairs of  77558\n",
            "Last loss: 1.863295316696167\n",
            "Processed  60800 QA pairs of  77558\n",
            "Last loss: 2.586892604827881\n",
            "Processed  60960 QA pairs of  77558\n",
            "Last loss: 2.2586989402770996\n",
            "Processed  61120 QA pairs of  77558\n",
            "Last loss: 1.940574049949646\n",
            "Processed  61280 QA pairs of  77558\n",
            "Last loss: 2.770036220550537\n",
            "Processed  61440 QA pairs of  77558\n",
            "Last loss: 2.771906614303589\n",
            "Processed  61600 QA pairs of  77558\n",
            "Last loss: 1.9763603210449219\n",
            "Processed  61760 QA pairs of  77558\n",
            "Last loss: 1.930769443511963\n",
            "Processed  61920 QA pairs of  77558\n",
            "Last loss: 1.6223655939102173\n",
            "Processed  62080 QA pairs of  77558\n",
            "Last loss: 2.852398157119751\n",
            "Processed  62240 QA pairs of  77558\n",
            "Last loss: 3.2317850589752197\n",
            "Processed  62400 QA pairs of  77558\n",
            "Last loss: 3.046463966369629\n",
            "Processed  62560 QA pairs of  77558\n",
            "Last loss: 1.857499599456787\n",
            "Processed  62720 QA pairs of  77558\n",
            "Last loss: 2.970289707183838\n",
            "Processed  62880 QA pairs of  77558\n",
            "Last loss: 2.999959945678711\n",
            "Processed  63040 QA pairs of  77558\n",
            "Last loss: 2.269563913345337\n",
            "Processed  63200 QA pairs of  77558\n",
            "Last loss: 2.1556453704833984\n",
            "Processed  63360 QA pairs of  77558\n",
            "Last loss: 1.975837230682373\n",
            "Processed  63520 QA pairs of  77558\n",
            "Last loss: 2.8057973384857178\n",
            "Processed  63680 QA pairs of  77558\n",
            "Last loss: 2.0279757976531982\n",
            "Processed  63840 QA pairs of  77558\n",
            "Last loss: 2.7872772216796875\n",
            "Processed  64000 QA pairs of  77558\n",
            "Last loss: 2.1260337829589844\n",
            "Processed  64160 QA pairs of  77558\n",
            "Last loss: 1.669704556465149\n",
            "Processed  64320 QA pairs of  77558\n",
            "Last loss: 1.8535971641540527\n",
            "Processed  64480 QA pairs of  77558\n",
            "Last loss: 2.6977620124816895\n",
            "Processed  64640 QA pairs of  77558\n",
            "Last loss: 2.1058855056762695\n",
            "Processed  64800 QA pairs of  77558\n",
            "Last loss: 2.3849217891693115\n",
            "Processed  64960 QA pairs of  77558\n",
            "Last loss: 1.7248566150665283\n",
            "Processed  65120 QA pairs of  77558\n",
            "Last loss: 2.1400680541992188\n",
            "Processed  65280 QA pairs of  77558\n",
            "Last loss: 1.872877836227417\n",
            "Processed  65440 QA pairs of  77558\n",
            "Last loss: 1.3523613214492798\n",
            "Processed  65600 QA pairs of  77558\n",
            "Last loss: 2.4930949211120605\n",
            "Processed  65760 QA pairs of  77558\n",
            "Last loss: 1.9568181037902832\n",
            "Processed  65920 QA pairs of  77558\n",
            "Last loss: 2.1543517112731934\n",
            "Processed  66080 QA pairs of  77558\n",
            "Last loss: 2.663332462310791\n",
            "Processed  66240 QA pairs of  77558\n",
            "Last loss: 1.1464133262634277\n",
            "Processed  66400 QA pairs of  77558\n",
            "Last loss: 2.5992231369018555\n",
            "Processed  66560 QA pairs of  77558\n",
            "Last loss: 2.171379804611206\n",
            "Processed  66720 QA pairs of  77558\n",
            "Last loss: 2.6925292015075684\n",
            "Processed  66880 QA pairs of  77558\n",
            "Last loss: 1.8874104022979736\n",
            "Processed  67040 QA pairs of  77558\n",
            "Last loss: 3.03486967086792\n",
            "Processed  67200 QA pairs of  77558\n",
            "Last loss: 1.6938061714172363\n",
            "Processed  67360 QA pairs of  77558\n",
            "Last loss: 1.3881969451904297\n",
            "Processed  67520 QA pairs of  77558\n",
            "Last loss: 1.9492015838623047\n",
            "Processed  67680 QA pairs of  77558\n",
            "Last loss: 2.2807672023773193\n",
            "Processed  67840 QA pairs of  77558\n",
            "Last loss: 1.3314846754074097\n",
            "Processed  68000 QA pairs of  77558\n",
            "Last loss: 2.8975603580474854\n",
            "Processed  68160 QA pairs of  77558\n",
            "Last loss: 3.0227746963500977\n",
            "Processed  68320 QA pairs of  77558\n",
            "Last loss: 2.1950764656066895\n",
            "Processed  68480 QA pairs of  77558\n",
            "Last loss: 2.489351272583008\n",
            "Processed  68640 QA pairs of  77558\n",
            "Last loss: 1.0843700170516968\n",
            "Processed  68800 QA pairs of  77558\n",
            "Last loss: 2.815932512283325\n",
            "Processed  68960 QA pairs of  77558\n",
            "Last loss: 2.4539384841918945\n",
            "Processed  69120 QA pairs of  77558\n",
            "Last loss: 2.5697507858276367\n",
            "Processed  69280 QA pairs of  77558\n",
            "Last loss: 2.369162082672119\n",
            "Processed  69440 QA pairs of  77558\n",
            "Last loss: 1.640679121017456\n",
            "Processed  69600 QA pairs of  77558\n",
            "Last loss: 1.963287591934204\n",
            "Processed  69760 QA pairs of  77558\n",
            "Last loss: 1.7980021238327026\n",
            "Processed  69920 QA pairs of  77558\n",
            "Last loss: 2.502349615097046\n",
            "Processed  70080 QA pairs of  77558\n",
            "Last loss: 0.9966850876808167\n",
            "Processed  70240 QA pairs of  77558\n",
            "Last loss: 2.148397207260132\n",
            "Processed  70400 QA pairs of  77558\n",
            "Last loss: 1.7138419151306152\n",
            "Processed  70560 QA pairs of  77558\n",
            "Last loss: 2.0540642738342285\n",
            "Processed  70720 QA pairs of  77558\n",
            "Last loss: 3.80466890335083\n",
            "Processed  70880 QA pairs of  77558\n",
            "Last loss: 1.7798212766647339\n",
            "Processed  71040 QA pairs of  77558\n",
            "Last loss: 1.5569278001785278\n",
            "Processed  71200 QA pairs of  77558\n",
            "Last loss: 2.3368067741394043\n",
            "Processed  71360 QA pairs of  77558\n",
            "Last loss: 1.689405918121338\n",
            "Processed  71520 QA pairs of  77558\n",
            "Last loss: 2.046971082687378\n",
            "Processed  71680 QA pairs of  77558\n",
            "Last loss: 1.7617475986480713\n",
            "Processed  71840 QA pairs of  77558\n",
            "Last loss: 1.5483763217926025\n",
            "Processed  72000 QA pairs of  77558\n",
            "Last loss: 1.9566318988800049\n",
            "Processed  72160 QA pairs of  77558\n",
            "Last loss: 2.221952438354492\n",
            "Processed  72320 QA pairs of  77558\n",
            "Last loss: 1.4600874185562134\n",
            "Processed  72480 QA pairs of  77558\n",
            "Last loss: 2.5383245944976807\n",
            "Processed  72640 QA pairs of  77558\n",
            "Last loss: 2.2131705284118652\n",
            "Processed  72800 QA pairs of  77558\n",
            "Last loss: 1.181917428970337\n",
            "Processed  72960 QA pairs of  77558\n",
            "Last loss: 2.43876314163208\n",
            "Processed  73120 QA pairs of  77558\n",
            "Last loss: 2.075807809829712\n",
            "Processed  73280 QA pairs of  77558\n",
            "Last loss: 2.199507474899292\n",
            "Processed  73440 QA pairs of  77558\n",
            "Last loss: 1.797966718673706\n",
            "Processed  73600 QA pairs of  77558\n",
            "Last loss: 2.110960006713867\n",
            "Processed  73760 QA pairs of  77558\n",
            "Last loss: 2.345017194747925\n",
            "Processed  73920 QA pairs of  77558\n",
            "Last loss: 2.855668067932129\n",
            "Processed  74080 QA pairs of  77558\n",
            "Last loss: 1.8510332107543945\n",
            "Processed  74240 QA pairs of  77558\n",
            "Last loss: 1.207015037536621\n",
            "Processed  74400 QA pairs of  77558\n",
            "Last loss: 0.948872447013855\n",
            "Processed  74560 QA pairs of  77558\n",
            "Last loss: 1.785839557647705\n",
            "Processed  74720 QA pairs of  77558\n",
            "Last loss: 1.5637413263320923\n",
            "Processed  74880 QA pairs of  77558\n",
            "Last loss: 2.3706564903259277\n",
            "Processed  75040 QA pairs of  77558\n",
            "Last loss: 2.469532012939453\n",
            "Processed  75200 QA pairs of  77558\n",
            "Last loss: 3.177039384841919\n",
            "Processed  75360 QA pairs of  77558\n",
            "Last loss: 1.8519773483276367\n",
            "Processed  75520 QA pairs of  77558\n",
            "Last loss: 1.5057487487792969\n",
            "Processed  75680 QA pairs of  77558\n",
            "Last loss: 3.923870086669922\n",
            "Processed  75840 QA pairs of  77558\n",
            "Last loss: 1.8952257633209229\n",
            "Processed  76000 QA pairs of  77558\n",
            "Last loss: 2.382053852081299\n",
            "Processed  76160 QA pairs of  77558\n",
            "Last loss: 3.6005172729492188\n",
            "Processed  76320 QA pairs of  77558\n",
            "Last loss: 2.1012258529663086\n",
            "Processed  76480 QA pairs of  77558\n",
            "Last loss: 2.5041427612304688\n",
            "Processed  76640 QA pairs of  77558\n",
            "Last loss: 3.3368849754333496\n",
            "Processed  76800 QA pairs of  77558\n",
            "Last loss: 2.98439884185791\n",
            "Processed  76960 QA pairs of  77558\n",
            "Last loss: 1.7601242065429688\n",
            "Processed  77120 QA pairs of  77558\n",
            "Last loss: 2.18912935256958\n",
            "Processed  77280 QA pairs of  77558\n",
            "Last loss: 1.175689458847046\n",
            "Processed  77440 QA pairs of  77558\n",
            "Last loss: 2.6727585792541504\n",
            "After epoch: 0 Loss is: 13443.138041317463\n"
          ]
        }
      ],
      "source": [
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00003)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    batch_counter = 0\n",
        "    for train_text_batch, train_span_batch, masks in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        train_text_batch, train_span_batch, masks = train_text_batch.to(device), train_span_batch.to(device), masks.to(device)\n",
        "        output = model(train_text_batch,attention_mask=masks)\n",
        "        loss = loss_function(output.start_logits, train_span_batch[:,0])\n",
        "        loss += loss_function(output.end_logits, train_span_batch[:,1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_counter += 1\n",
        "        if batch_counter % 10 == 0:\n",
        "            print(\"Processed \", batch_counter*batch_size, \"QA pairs of \", len(train_dataset))\n",
        "            print(\"Last loss:\", loss.item())\n",
        "        epoch_loss += loss.item()\n",
        "    print('After epoch:', epoch, 'Loss is:', epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbbr2MWdRMrW"
      },
      "source": [
        "### Exercise 2.2\n",
        "rubric={accuracy:2}\n",
        "\n",
        "Now run the trained classifier over the dev set and calculate the accuracy for each of the start and end predictions (independently). You should get above 60% performance for both. Don't forget to put the model in `eval` mode, with no gradients!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFleiJx7RMrW",
        "outputId": "4f227bda-aa48-4603-c483-ba78a34f1ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starts accuracy\n",
            "0.6235052955244278\n",
            "Ends accuracy\n",
            "0.6506662111376836\n"
          ]
        }
      ],
      "source": [
        "# Initialize empty lists to hold predicted and true start and end indices\n",
        "pred_starts = []\n",
        "true_starts = []\n",
        "pred_ends = []\n",
        "true_ends = []\n",
        "\n",
        "# Set the model to evaluation mode (disables dropout and batch normalization)\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculations to save memory and improve computation speed\n",
        "with torch.no_grad():\n",
        "    # Iterate over batches in the dataloader\n",
        "    for dev_text_batch, dev_span_batch, masks in dev_dataloader:\n",
        "        # Move input and mask tensors to the device (GPU or CPU)\n",
        "        dev_text_batch, masks = dev_text_batch.to(device), masks.to(device)\n",
        "\n",
        "        # Compute model output, including start and end logits for span prediction\n",
        "        output = model(dev_text_batch, attention_mask=masks)\n",
        "\n",
        "        # Extract start and end scores from the model's output\n",
        "        start_scores = output.start_logits\n",
        "        end_scores = output.end_logits\n",
        "\n",
        "        # Retrieve true start and end indices from the batch data\n",
        "        targets = dev_span_batch\n",
        "\n",
        "        # Predict indices by finding the position of the highest score and convert to numpy arrays\n",
        "        pred_starts.extend(torch.argmax(start_scores, dim=1).cpu().numpy())\n",
        "        pred_ends.extend(torch.argmax(end_scores, dim=1).cpu().numpy())\n",
        "\n",
        "        # Store the true start and end indices as numpy arrays\n",
        "        true_starts.extend(targets[:, 0].cpu().numpy())\n",
        "        true_ends.extend(targets[:, 1].cpu().numpy())\n",
        "\n",
        "# Output the accuracy of start and end index predictions\n",
        "print(\"Starts accuracy\")\n",
        "print(accuracy_score(true_starts, pred_starts))\n",
        "print(\"Ends accuracy\")\n",
        "print(accuracy_score(true_ends, pred_ends))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaT0PM3pRMro"
      },
      "source": [
        "## Exercise 3: Discrete optimization of answer spans\n",
        "rubric={accuracy:3, efficiency:2, quality:1}\n",
        "\n",
        "The model from exercise 2 independently predicts both start and end indices for the answer span. However, this is a case where there is a dependency between predictions that needs to be considered. In particular, it doesn't make sense to have the end index appear before the start index, or too long after it. You want to pick the highest probability pair that satisfies those basic constraints.\n",
        "\n",
        "You should write a function select_best_answer_span() which takes three arguments:\n",
        "\n",
        "* start_probs, a tensor of size num_examples x 512 giving the start of span log-probabilities.\n",
        "* end_probs, a tensor of size num_examples x 512 giving the end of span log-probabilities.\n",
        "* distance, a maximum distance between the start and end of the returned span. for each example, you should compute the start and end index for which the sum of log-probabilities is maximal provided that the start index < the end index and the distance between the indices is <= distance.\n",
        "\n",
        "Return a list of (start, end) pairs.  There are a few ways of doing this.  You can earn a bonus mark if you find a very efficient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9OrHpAxrRMro"
      },
      "outputs": [],
      "source": [
        "def select_best_answer_span(start_probs, end_probs, distance):\n",
        "    '''Given 2 matrices of probabilities associated with\n",
        "    indices of a text being the start or end of an answer spans, respectively,\n",
        "    solves the ILP with the objective function being the max probability,\n",
        "    under the restriction that the end index must be no more\n",
        "    than distance after the start. Returns a tuple (start index, end index)\n",
        "    corresponding to the best solution'''\n",
        "# Get the number of examples and the sequence length from the shape of start probabilities\n",
        "    num_examples, seq_length = start_probs.shape\n",
        "\n",
        "    best_spans = []\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        best_start, best_end = 0, 0\n",
        "        best_score = float('-inf')\n",
        "\n",
        "        for start in range(seq_length):\n",
        "            for end in range(start, min(start + distance + 1, seq_length)):\n",
        "                span_score = start_probs[i, start] + end_probs[i, end]\n",
        "\n",
        "                if span_score > best_score:\n",
        "                    best_start, best_end = start, end\n",
        "                    best_score = span_score\n",
        "\n",
        "        best_spans.append((best_start, best_end))\n",
        "\n",
        "    return best_spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogXXQd6rRMrq",
        "outputId": "c72990f3-3ec4-4ac7-d240-861dc31fbc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "test_starts = np.array([[0.1,0.5,0.2,0.1,0.1], [0.3,0.2,0.2,0.1,0.1]])\n",
        "test_ends = np.array([[0.4,0.1,0.3,0.1,0.1], [0.1,0.1,0.1,0.1,0.6]])\n",
        "assert select_best_answer_span(test_starts,test_ends,2) == [(1,2),(2,4)]\n",
        "print(\"Success!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKVDrLjlRMrj"
      },
      "source": [
        "## Exercise 4: Kaggle Competition\n",
        "\n",
        "### Exercise 4.1\n",
        "rubric={accuracy:2}\n",
        "\n",
        "Use your trained model from Exercise 2 to predict answers the test set. You will use your indices from `select_best_answer_span` to get a list of token ids, which you can use to slice the tensor and pass the tokens to the [decode](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.decode) method of your tokenizer, which will get you back to a string. Otherwise, you should be able to copy the dev prediction code from above. Create the usual output file (a file with header 'Id,Predicted', where 'Id' is a column of integers and 'Predicted' is the corresponding starting index (i.e. the \"tags\" of the past are the starting indices now), and submit to the Kaggle competition [here](https://www.kaggle.com/t/eb1a8df14a8c433a99200551852882f6). If you've done everything correctly you should be able to beat the baseline, which is required to get an A+ on this section."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YFvtgBS_VPy",
        "outputId": "70d366b6-0f95-413e-c9b0-66d18a62ee81"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIZADrjV__bD",
        "outputId": "bc89ce3e-4c12-4457-f7a1-53a3dad3b381"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import select_best_answer_span_v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "FgcbUpsR_ZWx",
        "outputId": "e4ef1ff0-d2b4-4b6a-ff34-c0e14aae0207"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'select_best_answer_span_v2' from 'evaluate' (/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-dae518e79b99>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_best_answer_span_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'select_best_answer_span_v2' from 'evaluate' (/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code here\n",
        "distance = 20\n",
        "predicted_starts = []\n",
        "# gold_starts = []\n",
        "predicted_ends = []\n",
        "# gold_ends = []\n",
        "model.eval()\n",
        "\n",
        "answers = []\n",
        "with torch.no_grad():\n",
        "    for test_text_batch, test_span_batch, masks in test_dataloader:\n",
        "        test_text_batch, masks = test_text_batch.to(device), masks.to(device)\n",
        "        output = model(test_text_batch, attention_mask=masks)\n",
        "\n",
        "        # Copy from Exercise 2.2\n",
        "        start_scores = output.start_logits\n",
        "        end_scores = output.end_logits\n",
        "\n",
        "        # Start_probs and end_probs by `F.log_softmax` of start_scores and end_scores\n",
        "        start_probs = F.log_softmax(start_scores, dim=1).cpu().numpy()\n",
        "        end_probs = F.log_softmax(end_scores, dim=1).cpu().numpy()\n",
        "\n",
        "        # Find your spans using Ex3.1\n",
        "        spans = select_best_answer_span(start_probs, end_probs, distance)\n",
        "\n",
        "        # Append text to answers\n",
        "        for i in range(len(spans)):\n",
        "            # Using `tokenizer.decode` for `test_text_batch` to append to `answers`\n",
        "            text_ids = test_text_batch[i][spans[i][0]:spans[i][1]+1].cpu().numpy()\n",
        "            decoded_text = tokenizer.decode(text_ids)\n",
        "            answers.append(decoded_text)\n",
        "\n",
        "# Save to the file\n",
        "with open(\"/content/gdrive/MyDrive/Lab3/test_answers.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
        "    fout.write('Id,Predicted\\n')\n",
        "    for idx, pred in enumerate(answers):\n",
        "        fout.write(str(idx) + ',\"' + str(pred).replace('\"', '\"\"') + '\"\\n')"
      ],
      "metadata": {
        "id": "gsr1CrtZ1hCd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95fuB3S7RMrn"
      },
      "source": [
        "### Exercise 4.2 (Optional)\n",
        "rubric={raw:2}\n",
        "\n",
        "Now compete to improve your QA system. The two easiest ways to improve your score are to tune the hyperparameters and to use a more powerful version of BERT. You can do some research to find out which flavor is currently the best on this task.\n",
        "\n",
        "As usual, points in this exercise will be given based on ranking in the *Private* leaderboard at the deadline.\n",
        "\n",
        "- 1st: 2 points\n",
        "- 2nd: 1.8 point\n",
        "- 3rd: 1.6 point\n",
        "- 4th: 1.4 point\n",
        "- 5th: 1.2 point\n",
        "- 6th: 1 point\n",
        "-\n",
        "Again, make sure we can duplicate your result by running your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDvZ09DXd-Na"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db0efb44c88d4b2ca847dc57ee9a7616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_350b6b4cb9ea43dc8cf84b46410affe4",
              "IPY_MODEL_a9f8b6a986954f388580aa27d38f4541",
              "IPY_MODEL_6ab7b9d218764991914c8accb2446045"
            ],
            "layout": "IPY_MODEL_10cf0efbd8d24c7aad7be5f1a16b5b2a"
          }
        },
        "350b6b4cb9ea43dc8cf84b46410affe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e4b49d5ea6f4c74a318eb96fd705a0d",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0fa97de72d4f51a2081b3921aab4c0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a9f8b6a986954f388580aa27d38f4541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdb6a3c5564a4df1a4b7b465870a87e2",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a7f4611b54b4973ba313e7dfe380ab9",
            "value": 28
          }
        },
        "6ab7b9d218764991914c8accb2446045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a429155131e34595939507f1122e6d8f",
            "placeholder": "​",
            "style": "IPY_MODEL_9034be4769804ec5b220bd4617f35a9b",
            "value": " 28.0/28.0 [00:00&lt;00:00, 808B/s]"
          }
        },
        "10cf0efbd8d24c7aad7be5f1a16b5b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e4b49d5ea6f4c74a318eb96fd705a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0fa97de72d4f51a2081b3921aab4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdb6a3c5564a4df1a4b7b465870a87e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7f4611b54b4973ba313e7dfe380ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a429155131e34595939507f1122e6d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9034be4769804ec5b220bd4617f35a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42ffc530955a4cfaa2bd32f4bed08b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acaaa5d0625c43d998e975f9c9b6b551",
              "IPY_MODEL_87f468138ffc4884acb4fcf4ea0b0409",
              "IPY_MODEL_c26226e928234d1f9f700db4e3285f69"
            ],
            "layout": "IPY_MODEL_e6d3613560d34b7e871b60cf2af9fe5a"
          }
        },
        "acaaa5d0625c43d998e975f9c9b6b551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89e61dd4d25b42c58a3e622ae8e615b2",
            "placeholder": "​",
            "style": "IPY_MODEL_50d5b5790a9d44ecba5a587de4b898d8",
            "value": "vocab.txt: 100%"
          }
        },
        "87f468138ffc4884acb4fcf4ea0b0409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d7c60a4943f4297807b3d099a5d84f3",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad32882649d344899fe5866a7d96a037",
            "value": 231508
          }
        },
        "c26226e928234d1f9f700db4e3285f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64e61084154c4f78b0b1b8ea1a30df41",
            "placeholder": "​",
            "style": "IPY_MODEL_1edf6d730130464c91e79fd902d4b535",
            "value": " 232k/232k [00:00&lt;00:00, 6.18MB/s]"
          }
        },
        "e6d3613560d34b7e871b60cf2af9fe5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89e61dd4d25b42c58a3e622ae8e615b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d5b5790a9d44ecba5a587de4b898d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d7c60a4943f4297807b3d099a5d84f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad32882649d344899fe5866a7d96a037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64e61084154c4f78b0b1b8ea1a30df41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edf6d730130464c91e79fd902d4b535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92dfe776feda48aea3d95b7d9280810b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddc2b7705ad54d7b99a5d98094610628",
              "IPY_MODEL_02393b909e854145b8069fae46b9888c",
              "IPY_MODEL_b45bc83c30004b878de748bf63575733"
            ],
            "layout": "IPY_MODEL_8f9c725caa7843ed8583145eb2fa1fa5"
          }
        },
        "ddc2b7705ad54d7b99a5d98094610628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7e599bb428549a5bc25595099317be3",
            "placeholder": "​",
            "style": "IPY_MODEL_15754d92befa49a0801e69b7b1dd631c",
            "value": "tokenizer.json: 100%"
          }
        },
        "02393b909e854145b8069fae46b9888c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02dd6ae7c74f4a8c850423b57036b1be",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a4c49e57d29429391b36954fa09ef01",
            "value": 466062
          }
        },
        "b45bc83c30004b878de748bf63575733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9164932a6f384bc797714068b09fef58",
            "placeholder": "​",
            "style": "IPY_MODEL_72fec13a789149e4bb355e263d71b982",
            "value": " 466k/466k [00:00&lt;00:00, 1.87MB/s]"
          }
        },
        "8f9c725caa7843ed8583145eb2fa1fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7e599bb428549a5bc25595099317be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15754d92befa49a0801e69b7b1dd631c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02dd6ae7c74f4a8c850423b57036b1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4c49e57d29429391b36954fa09ef01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9164932a6f384bc797714068b09fef58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72fec13a789149e4bb355e263d71b982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45b12882f5b1485b8a21c854c390d6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72543cf3a9064c19bce891399ad5ce84",
              "IPY_MODEL_0ccf35c1a1ee410fa45d453615c471f2",
              "IPY_MODEL_5d376bf0dc1e409e8f147461c4ba841d"
            ],
            "layout": "IPY_MODEL_842044bb6708415cbfb2e72142692e07"
          }
        },
        "72543cf3a9064c19bce891399ad5ce84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e81f5c53ec747aa8a3f899bb7dc3915",
            "placeholder": "​",
            "style": "IPY_MODEL_a2b5a5a34781460290f0e93804264fa8",
            "value": "config.json: 100%"
          }
        },
        "0ccf35c1a1ee410fa45d453615c471f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb67124fc3824246813a53f446d2d23b",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0e664dc338a4cca89a037d4a4cdc278",
            "value": 483
          }
        },
        "5d376bf0dc1e409e8f147461c4ba841d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47dc0209a5c2437fa25caf1208fa5c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_51d945bcf5a14f92a2b130ef03e2000b",
            "value": " 483/483 [00:00&lt;00:00, 7.67kB/s]"
          }
        },
        "842044bb6708415cbfb2e72142692e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e81f5c53ec747aa8a3f899bb7dc3915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b5a5a34781460290f0e93804264fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb67124fc3824246813a53f446d2d23b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e664dc338a4cca89a037d4a4cdc278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47dc0209a5c2437fa25caf1208fa5c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d945bcf5a14f92a2b130ef03e2000b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a909e8259649acb4460e841c4b3a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80c9238632a54c07aa6971d6a239d6f8",
              "IPY_MODEL_1d695de50a0742929f794b6b4ad480b2",
              "IPY_MODEL_9973d359bd53494e952e394e71330342"
            ],
            "layout": "IPY_MODEL_2ee2cbcb5ed9421ebaf82e69675da116"
          }
        },
        "80c9238632a54c07aa6971d6a239d6f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a813d46881fc4fd0a2cb9558553caed9",
            "placeholder": "​",
            "style": "IPY_MODEL_c14df920692a4cf1bf3af48e16b33dd5",
            "value": "model.safetensors: 100%"
          }
        },
        "1d695de50a0742929f794b6b4ad480b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_469e3e991bb64a6f99e1084f0303daf3",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8817c0928ba548aa8c0fcb48cb985a27",
            "value": 267954768
          }
        },
        "9973d359bd53494e952e394e71330342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f218da0c32b460483642caeab395fac",
            "placeholder": "​",
            "style": "IPY_MODEL_eace7a097f3b47eb9a2b71b52f3a4472",
            "value": " 268M/268M [00:02&lt;00:00, 98.5MB/s]"
          }
        },
        "2ee2cbcb5ed9421ebaf82e69675da116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a813d46881fc4fd0a2cb9558553caed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14df920692a4cf1bf3af48e16b33dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "469e3e991bb64a6f99e1084f0303daf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8817c0928ba548aa8c0fcb48cb985a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f218da0c32b460483642caeab395fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eace7a097f3b47eb9a2b71b52f3a4472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}